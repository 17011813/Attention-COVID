# -*- coding: utf-8 -*-
"""1일.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vgtz3Z3hjC-FT4fcak6ea2EKv8ReveYV
"""

from google.colab import drive
drive.mount('/content/gdrive/')

import tensorflow as tf 
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd 
import seaborn as sns
from sklearn.metrics import mean_squared_error
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import backend as K
from sklearn.metrics import mean_absolute_error
def plcc_metric(y_true, y_pred):  
    mx = K.mean(K.constant(y_true))
    my = K.mean(K.constant(y_pred))
    xm, ym = y_true-mx, y_pred-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym)))) + 1e-12
    return r_num / r_den

df = pd.read_csv("/content/gdrive/MyDrive/tr.csv" , encoding='cp949')
Date = df.iloc[:,0]
df = df.iloc[:, 1:]                   # 맨 앞의 Date는 제외

#df = df.transpose()                  # 행이랑 열 바꿔서 피쳐로 해보자 

from sklearn.preprocessing import MinMaxScaler                    # 얘를 하면 들어온 애의 shape 그대로 다시 back 할 수 있구나
sc = MinMaxScaler()

df = df.interpolate()       # 선형 보간

xy = df.values        

window = 5                          # window size
trainSize = 320                       # 80%만 train

trainx = sc.fit_transform((xy[0:trainSize,1:]))
trainy = sc.fit_transform((xy[0:trainSize,[0]]))
testx = sc.fit_transform(xy[trainSize-window:,1:])
testy = sc.fit_transform((xy[trainSize-window:,[0]]))

trainSet = np.concatenate((trainy, trainx), axis=1)
testSet = np.concatenate((testy, testx), axis=1)

predict_day = 1

def buildDataSet(data, window):
    xdata = []
    ydata = []
    for i in range(0, len(data) - window - predict_day):
        xdata.append(data[i:i + window])                         
        ydata.append(data[i + window + predict_day - 1,[0]])                   # 행은 그 다음 행 하나랑, 열은 Number만
    return np.array(xdata), np.array(ydata)
    
lstm_trainX, trainY=buildDataSet(trainSet, window)
lstm_testX, testY=buildDataSet(testSet, window)

trainX = np.transpose(lstm_trainX,(0,2,1))              # Attention에 넣어줄 애만 전치
testX = np.transpose(lstm_testX,(0,2,1))                # 5,31 ---> 31,5 로 전치


trainX.shape,trainY.shape,testX.shape,testY.shape

plt.figure(figsize=(16, 9))
plt.plot(xy[trainSize-window:,[0]], label = "정규화 전 Actual Test Y")

sum_predict = 0
for i in range(5):
  model = keras.Sequential()

  model.add(layers.LSTM(units=32, input_shape=[5,31]))     # LSTM에서는 5,31로 lstm_trainX로 넣어준다
  model.add(layers.Dense(1))    # 하나 나온 값이 예측한 주가
  model.compile(loss='mse', optimizer='adam', metrics=['mae'])

  model.fit(lstm_trainX, trainY, epochs=30, batch_size=16)
  prediction = model.predict(lstm_testX)
  sum_predict += prediction

sum_predict = sum_predict/5
actual = sc.inverse_transform(testY.reshape(-1,1))
predict = sc.inverse_transform(sum_predict.reshape(-1,1))

print("RMSE Evaluate : {}".format(mean_squared_error(actual, predict)**0.5)) 
print("PCC : {}".format(plcc_metric(actual,predict)))
plt.figure(figsize=(16, 9))
plt.plot(Date.iloc[trainSize+predict_day:], predict, label = "LSTM predictions")
plt.plot(actual,label = "Actual data")
plt.xticks(np.arange(0,240,60), fontsize=20)
plt.yticks(fontsize=20)
plt.title('COVID-19 Forecasting After 1 day',fontsize=20)
plt.xlabel('Date', fontsize=20)
plt.ylabel('No. of Confirmed Cases' ,fontsize=20)
plt.legend(prop={'size': 30})

import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

TIME_STEPS = 31
INPUT_DIM  = 5

def attention_3d_block(inputs):
    
    # inputs.shape = (batch_size, time_steps, input_dim)
    input_dim = int(inputs.shape[2])
    
    a = tf.keras.layers.Permute((2, 1))(inputs) # same transpose
    #a = tf.keras.layers.Reshape((input_dim, TIME_STEPS))(a) 

    a = tf.keras.layers.Dense(TIME_STEPS, activation='softmax')(a)
    
    a_probs = tf.keras.layers.Permute((2, 1), name='attention_vec')(a)
    
    output_attention_mul  = tf.keras.layers.multiply([inputs, a_probs])
    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
    return output_attention_mul

def model_attention_applied_after_lstm():
    
    inputs        = tf.keras.Input(shape=(TIME_STEPS, INPUT_DIM,))

    lstm_out      = tf.keras.layers.LSTM(units = 32, return_sequences=True)(inputs)
    
    attention_mul = attention_3d_block(lstm_out)
    attention_mul = tf.keras.layers.Flatten()(attention_mul)
    
    output        = tf.keras.layers.Dense(1, activation='tanh')(attention_mul)
    
    model         = tf.keras.Model(inputs=[inputs], outputs=output)
    
    return model

def model_attention_applied_before_lstm():
    
    inputs        = tf.keras.Input(shape=(TIME_STEPS, INPUT_DIM,))
    
    attention_mul = attention_3d_block(inputs)
    attention_mul = tf.keras.layers.LSTM(units = 32, return_sequences=False)(attention_mul)    
    output        = tf.keras.layers.Dense(1, activation='tanh')(attention_mul)
    
    model         = tf.keras.Model(inputs=[inputs], outputs=output)
    
    return model

sum_predict2 = 0
for i in range(5):
  model2 = model_attention_applied_after_lstm()
  model2.compile(loss='mse', optimizer='adam', metrics=['mae'])

  model2.fit(trainX, trainY, epochs=30, batch_size=16)
  prediction2 = model2.predict(testX)
  sum_predict2 += prediction2

sum_predict2 = sum_predict2/5
predict2 = sc.inverse_transform(sum_predict2.reshape(-1,1))

print("RMSE Evaluate : {}".format(mean_squared_error(actual, predict2)**0.5)) 
print("PCC : {}".format(plcc_metric(actual,predict2)))
plt.figure(figsize=(16, 9))
plt.plot(Date.iloc[trainSize+predict_day:], predict2, label = "LSTM-Attention predictions")
plt.plot(actual,label = "Actual data")
plt.xticks(np.arange(0,250,50), fontsize=20)
plt.yticks(fontsize=20)
plt.title('COVID-19 Forecasting After 1 day',fontsize=20)
plt.xlabel('Date', fontsize=20)
plt.ylabel('No. of Confirmed Cases' ,fontsize=20)
plt.legend(prop={'size': 30})

sum_predict3 = 0
for i in range(5):  
  model3 = model_attention_applied_before_lstm()
  model3.compile(loss='mse', optimizer='adam', metrics=['mae'])

  model3.fit(trainX, trainY, epochs=30, batch_size=16)
  prediction3 = model3.predict(testX)
  sum_predict3 += prediction3

sum_predict3 = sum_predict3/5
predict3 = sc.inverse_transform(sum_predict3.reshape(-1,1))

print("RMSE Evaluate : {}".format(mean_squared_error(actual, predict3)**0.5)) 
print("PCC : {}".format(plcc_metric(actual,predict3)))
plt.figure(figsize=(16, 9))
plt.plot(Date.iloc[trainSize+predict_day:], predict3, label = "Attention-LSTM predictions")
plt.plot(actual,label = "Actual data")
plt.xticks(np.arange(0,250,50), fontsize=20)
plt.yticks(fontsize=20)
plt.title('COVID-19 Forecasting After 1 day',fontsize=20)
plt.xlabel('Date', fontsize=20)
plt.ylabel('No. of Confirmed Cases' ,fontsize=20)
plt.legend(prop={'size': 30})

print("RMSE Evaluate : {}".format(mean_squared_error(actual, predict3)**0.5)) 
print("PCC : {}".format(plcc_metric(actual,predict3)))
plt.figure(figsize=(16, 9))
plt.plot(Date.iloc[trainSize+predict_day:],predict3, label = "Attention-LSTM predictions")
plt.plot(actual,label = "Actual data")
plt.xticks(np.arange(0,240,60), fontsize=20)
plt.yticks(fontsize=20)
plt.title('COVID-19 Forecasting After 1 day',fontsize=20)
plt.xlabel('Date', fontsize=20)
plt.ylabel('No. of Confirmed Cases' ,fontsize=20)
plt.legend(prop={'size': 30})

def get_data_recurrent(n, time_steps, input_dim, attention_column=30):

    x = np.random.standard_normal(size=(n, time_steps, input_dim))
    y = np.random.randint(low=0, high=2, size=(n, 1))

    x[:, attention_column, :] = np.tile(y, input_dim)

    return x, y

train_x, train_y = get_data_recurrent(300000, TIME_STEPS, INPUT_DIM)

modelman = model_attention_applied_before_lstm()
#modelman = model_attention_applied_after_lstm()
modelman.compile(optimizer='adam', loss='mse', metrics=['mae'])

modelman.fit([train_x], train_y, epochs=1, batch_size=16, validation_split=0.1)

layer_outputs    = [layer.output for layer in modelman.layers if layer.name == 'attention_vec']
activation_model = tf.keras.models.Model(inputs=modelman.input, outputs=layer_outputs)

attention_vectors = []
for i in range(50):
    test_x, test_y = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)

    predict_output   = activation_model.predict(test_x)
    #print(predict_output.shape)
    dols = np.mean(predict_output, axis=2).squeeze()
    #print(dols,"DOLS",dols.shape)
    
    #assert (np.sum(dols) - 1.0) < 1e-5
    #attention_vectors.append
    attention_vectors.append(dols)

attention_vector_final = np.mean(np.array(attention_vectors), axis=0)

pd.DataFrame(attention_vector_final, columns=['Attention (%)']).plot.bar()

